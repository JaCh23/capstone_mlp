{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaCh23/capstone_mlp/blob/master/pca_mlp_jupyter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyQr0CGVJioF",
        "outputId": "9a79631b-747d-41ba-f228-392e4e6964e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SocJe8IJ0TI9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "lwPRzT-mGxgJ"
      },
      "outputs": [],
      "source": [
        "# Read the CSV file into a DataFrame\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/final data/raw_sensor_values_sample_v6_new_logout_strangers_teammates.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "6ieG_NxxPkzg",
        "outputId": "ca2164e1-29a5-4550-9638-933daf9af764"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        0     1     2     3      4      5     6     7     8     9  ...   171  \\\n",
              "0   -0.04  0.03  0.02  0.92  -9.87 -10.88 -0.03  0.11  0.06  0.96  ...  3.60   \n",
              "1   -0.06  0.03 -0.03  0.86  -9.59  -9.81 -0.05  0.13 -0.01  1.19  ...  4.12   \n",
              "2   -0.07  0.10  0.00  1.06  -9.63  -9.24 -0.07  0.10  0.02  1.02  ...  3.57   \n",
              "3    0.00  0.05  0.01 -1.38  -9.23 -10.24 -0.14 -0.16  0.24 -0.68  ...  0.96   \n",
              "4   -0.11  0.03 -0.08  1.13  -9.52  -8.93 -0.11  0.07 -0.01  0.94  ...  5.66   \n",
              "..    ...   ...   ...   ...    ...    ...   ...   ...   ...   ...  ...   ...   \n",
              "115 -0.07  0.01 -0.02  1.11  -9.56  -9.06 -0.16  0.10  0.02  1.41  ...  2.80   \n",
              "116  0.02 -0.02 -0.03  5.73  -7.62 -12.79  0.03 -0.05 -0.04  5.86  ...  6.04   \n",
              "117 -0.03  0.05  0.00  5.07  -7.93 -12.68 -0.01  0.03 -0.01  5.13  ...  5.88   \n",
              "118 -0.04  0.00 -0.21 -0.32 -10.01 -10.57  0.00 -0.14 -0.15 -0.62  ...  2.64   \n",
              "119 -0.02  0.01  0.01  0.94  -9.57 -12.20 -0.02  0.01 -0.01  0.99  ...  9.50   \n",
              "\n",
              "       172    173   174   175   176   177    178    179  label  \n",
              "0    -8.29  -1.97  0.29 -0.16 -0.11  3.27  -7.98  -4.51      G  \n",
              "1   -13.99  -4.38 -1.10 -0.56  1.34  6.46 -14.71  -3.88      G  \n",
              "2   -11.03  -1.93 -2.04 -0.51  0.96  1.19 -14.25  -1.64      G  \n",
              "3   -15.50  -3.72 -2.47 -0.16  1.77  5.98 -19.56   0.53      G  \n",
              "4    -8.13   3.06 -1.05 -0.40  0.51  2.84  -7.73   0.32      G  \n",
              "..     ...    ...   ...   ...   ...   ...    ...    ...    ...  \n",
              "115  -6.71   3.10 -0.03 -0.12  0.06  3.82  -6.72   1.19      G  \n",
              "116  -7.54 -12.91  0.02  0.00 -0.05  5.57  -7.67 -12.87      G  \n",
              "117  -7.42 -12.80  0.03 -0.05 -0.03  5.86  -7.16 -12.85      G  \n",
              "118 -10.00  -2.57 -0.03 -0.22  0.00  3.07  -9.72  -3.37      G  \n",
              "119  -9.68 -12.21 -0.01  0.18 -0.21  2.53  -8.46 -13.40      G  \n",
              "\n",
              "[120 rows x 181 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c800cd5-c877-4656-b4cf-145aa3b9c8e0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>171</th>\n",
              "      <th>172</th>\n",
              "      <th>173</th>\n",
              "      <th>174</th>\n",
              "      <th>175</th>\n",
              "      <th>176</th>\n",
              "      <th>177</th>\n",
              "      <th>178</th>\n",
              "      <th>179</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.03</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.92</td>\n",
              "      <td>-9.87</td>\n",
              "      <td>-10.88</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.11</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.96</td>\n",
              "      <td>...</td>\n",
              "      <td>3.60</td>\n",
              "      <td>-8.29</td>\n",
              "      <td>-1.97</td>\n",
              "      <td>0.29</td>\n",
              "      <td>-0.16</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>3.27</td>\n",
              "      <td>-7.98</td>\n",
              "      <td>-4.51</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.06</td>\n",
              "      <td>0.03</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.86</td>\n",
              "      <td>-9.59</td>\n",
              "      <td>-9.81</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.13</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>1.19</td>\n",
              "      <td>...</td>\n",
              "      <td>4.12</td>\n",
              "      <td>-13.99</td>\n",
              "      <td>-4.38</td>\n",
              "      <td>-1.10</td>\n",
              "      <td>-0.56</td>\n",
              "      <td>1.34</td>\n",
              "      <td>6.46</td>\n",
              "      <td>-14.71</td>\n",
              "      <td>-3.88</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.06</td>\n",
              "      <td>-9.63</td>\n",
              "      <td>-9.24</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.02</td>\n",
              "      <td>1.02</td>\n",
              "      <td>...</td>\n",
              "      <td>3.57</td>\n",
              "      <td>-11.03</td>\n",
              "      <td>-1.93</td>\n",
              "      <td>-2.04</td>\n",
              "      <td>-0.51</td>\n",
              "      <td>0.96</td>\n",
              "      <td>1.19</td>\n",
              "      <td>-14.25</td>\n",
              "      <td>-1.64</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-1.38</td>\n",
              "      <td>-9.23</td>\n",
              "      <td>-10.24</td>\n",
              "      <td>-0.14</td>\n",
              "      <td>-0.16</td>\n",
              "      <td>0.24</td>\n",
              "      <td>-0.68</td>\n",
              "      <td>...</td>\n",
              "      <td>0.96</td>\n",
              "      <td>-15.50</td>\n",
              "      <td>-3.72</td>\n",
              "      <td>-2.47</td>\n",
              "      <td>-0.16</td>\n",
              "      <td>1.77</td>\n",
              "      <td>5.98</td>\n",
              "      <td>-19.56</td>\n",
              "      <td>0.53</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.11</td>\n",
              "      <td>0.03</td>\n",
              "      <td>-0.08</td>\n",
              "      <td>1.13</td>\n",
              "      <td>-9.52</td>\n",
              "      <td>-8.93</td>\n",
              "      <td>-0.11</td>\n",
              "      <td>0.07</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.94</td>\n",
              "      <td>...</td>\n",
              "      <td>5.66</td>\n",
              "      <td>-8.13</td>\n",
              "      <td>3.06</td>\n",
              "      <td>-1.05</td>\n",
              "      <td>-0.40</td>\n",
              "      <td>0.51</td>\n",
              "      <td>2.84</td>\n",
              "      <td>-7.73</td>\n",
              "      <td>0.32</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>115</th>\n",
              "      <td>-0.07</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>1.11</td>\n",
              "      <td>-9.56</td>\n",
              "      <td>-9.06</td>\n",
              "      <td>-0.16</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.02</td>\n",
              "      <td>1.41</td>\n",
              "      <td>...</td>\n",
              "      <td>2.80</td>\n",
              "      <td>-6.71</td>\n",
              "      <td>3.10</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>-0.12</td>\n",
              "      <td>0.06</td>\n",
              "      <td>3.82</td>\n",
              "      <td>-6.72</td>\n",
              "      <td>1.19</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>116</th>\n",
              "      <td>0.02</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>5.73</td>\n",
              "      <td>-7.62</td>\n",
              "      <td>-12.79</td>\n",
              "      <td>0.03</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>-0.04</td>\n",
              "      <td>5.86</td>\n",
              "      <td>...</td>\n",
              "      <td>6.04</td>\n",
              "      <td>-7.54</td>\n",
              "      <td>-12.91</td>\n",
              "      <td>0.02</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>5.57</td>\n",
              "      <td>-7.67</td>\n",
              "      <td>-12.87</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>-0.03</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.00</td>\n",
              "      <td>5.07</td>\n",
              "      <td>-7.93</td>\n",
              "      <td>-12.68</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.03</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>5.13</td>\n",
              "      <td>...</td>\n",
              "      <td>5.88</td>\n",
              "      <td>-7.42</td>\n",
              "      <td>-12.80</td>\n",
              "      <td>0.03</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>5.86</td>\n",
              "      <td>-7.16</td>\n",
              "      <td>-12.85</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>118</th>\n",
              "      <td>-0.04</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.21</td>\n",
              "      <td>-0.32</td>\n",
              "      <td>-10.01</td>\n",
              "      <td>-10.57</td>\n",
              "      <td>0.00</td>\n",
              "      <td>-0.14</td>\n",
              "      <td>-0.15</td>\n",
              "      <td>-0.62</td>\n",
              "      <td>...</td>\n",
              "      <td>2.64</td>\n",
              "      <td>-10.00</td>\n",
              "      <td>-2.57</td>\n",
              "      <td>-0.03</td>\n",
              "      <td>-0.22</td>\n",
              "      <td>0.00</td>\n",
              "      <td>3.07</td>\n",
              "      <td>-9.72</td>\n",
              "      <td>-3.37</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119</th>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.94</td>\n",
              "      <td>-9.57</td>\n",
              "      <td>-12.20</td>\n",
              "      <td>-0.02</td>\n",
              "      <td>0.01</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.99</td>\n",
              "      <td>...</td>\n",
              "      <td>9.50</td>\n",
              "      <td>-9.68</td>\n",
              "      <td>-12.21</td>\n",
              "      <td>-0.01</td>\n",
              "      <td>0.18</td>\n",
              "      <td>-0.21</td>\n",
              "      <td>2.53</td>\n",
              "      <td>-8.46</td>\n",
              "      <td>-13.40</td>\n",
              "      <td>G</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>120 rows Ã— 181 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c800cd5-c877-4656-b4cf-145aa3b9c8e0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7c800cd5-c877-4656-b4cf-145aa3b9c8e0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7c800cd5-c877-4656-b4cf-145aa3b9c8e0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# Sort the DataFrame by the 'label' column in alphabetical order\n",
        "df = df.sort_values(by='label')\n",
        "\n",
        "# Reset the index and drop the old index column\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "df.head(120)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsykLTFK4zKc",
        "outputId": "58361a9f-1cf7-4e1c-e01c-c11b7ad76320"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "G    210\n",
            "L    210\n",
            "R    210\n",
            "S    210\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# find all unique values in the 'labels' column\n",
        "new_df = df\n",
        "\n",
        "# Get the count of each label\n",
        "label_counts = new_df['label'].value_counts()\n",
        "\n",
        "print(label_counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "hiiMCrz4Dvqb"
      },
      "outputs": [],
      "source": [
        "# 360 -> 90x G,L,R,S\n",
        "\n",
        "SAMPLES_PER_LABEL = 210\n",
        "\n",
        "test_g = new_df.iloc[1].values\n",
        "test_g = test_g[:-1]\n",
        "\n",
        "test_l = new_df.iloc[(1*SAMPLES_PER_LABEL) + 1].values\n",
        "test_l = test_l[:-1]\n",
        "\n",
        "test_r = new_df.iloc[(2*SAMPLES_PER_LABEL) + 1].values\n",
        "test_r = test_r[:-1]\n",
        "\n",
        "test_s = new_df.iloc[(3*SAMPLES_PER_LABEL) + 1].values\n",
        "test_s = test_s[:-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06fQQzXvwr1s",
        "outputId": "eb4c1275-7c7c-444a-8d1e-0fd77d24fd4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-hot encoding scheme: [array([0, 1, 2, 3])]\n",
            "Mapping of numerical labels to original labels: {0: 'G', 1: 'L', 2: 'R', 3: 'S'}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# Instantiate LabelEncoder and OneHotEncoder\n",
        "le = LabelEncoder()\n",
        "ohe = OneHotEncoder()\n",
        "\n",
        "# Encode the labels of the training set\n",
        "labels_encoded = le.fit_transform(new_df.iloc[:, -1])\n",
        "# Convert the encoded labels to one-hot encoding\n",
        "labels_onehot = ohe.fit_transform(labels_encoded.reshape(-1, 1)).toarray()\n",
        "\n",
        "# Print the one-hot encoding scheme and mapping\n",
        "print('One-hot encoding scheme:', ohe.categories_)\n",
        "print('Mapping of numerical labels to original labels:', {i:label for i, label in enumerate(le.classes_)})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb-oVPehupaE"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(new_df.iloc[:, :-1].astype(float), labels_onehot, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAjrNauI9E8H"
      },
      "outputs": [],
      "source": [
        "from scipy.signal import find_peaks\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "def extract_features(raw_sensor_data):\n",
        "\n",
        "    # Apply median filtering column-wise using the rolling function, window=5\n",
        "    # min_periods=1 argument allows the rolling mean to be calculated even if there are fewer than 5 rows available, by using all the available rows in window instead\n",
        "    sensor_data = raw_sensor_data.rolling(5, min_periods=1, axis=0).mean()\n",
        "    sensor_data = sensor_data.to_numpy()\n",
        "\n",
        "    # Compute statistical features\n",
        "    mean = np.mean(sensor_data, axis=0)\n",
        "    std = np.std(sensor_data, axis=0)\n",
        "    abs_diff = np.abs(np.diff(sensor_data, axis=0)).mean(axis=0)\n",
        "    minimum = np.min(sensor_data, axis=0)\n",
        "    maximum = np.max(sensor_data, axis=0)\n",
        "    max_min_diff = maximum - minimum\n",
        "    median = np.median(sensor_data, axis=0)\n",
        "    mad = np.median(np.abs(sensor_data - np.median(sensor_data, axis=0)), axis=0)\n",
        "    iqr = np.percentile(sensor_data, 75, axis=0) - np.percentile(sensor_data, 25, axis=0)\n",
        "    negative_count = np.sum(sensor_data < 0, axis=0)\n",
        "    positive_count = np.sum(sensor_data > 0, axis=0)\n",
        "    values_above_mean = np.sum(sensor_data > mean, axis=0)\n",
        "    \n",
        "    peak_counts = np.array(np.apply_along_axis(lambda x: len(find_peaks(x)[0]), 0, sensor_data)).flatten()\n",
        "\n",
        "    skewness = np.array(pd.DataFrame(sensor_data.reshape(-1,6)).skew().values).flatten()\n",
        "    kurt = np.array(pd.DataFrame(sensor_data.reshape(-1,6)).kurtosis().values).flatten()\n",
        "    energy = np.array(np.sum(sensor_data**2, axis=0)).flatten()\n",
        "\n",
        "    # Compute the average resultant for gyro and acc columns\n",
        "    gyro_cols = sensor_data[:, :3]\n",
        "    acc_cols = sensor_data[:, 3:]\n",
        "    gyro_avg_result = np.array(np.sqrt((gyro_cols**2).sum(axis=1)).mean()).flatten()\n",
        "    acc_avg_result = np.array(np.sqrt((acc_cols**2).sum(axis=1)).mean()).flatten()\n",
        "\n",
        "    # Compute the signal magnitude area for gyro and acc columns\n",
        "    gyro_sma = np.array((np.abs(gyro_cols) / 100).sum(axis=0).sum()).flatten()\n",
        "    acc_sma = np.array((np.abs(acc_cols) / 100).sum(axis=0).sum()).flatten()\n",
        "\n",
        "    # Concatenate features and return as a list\n",
        "    temp_features = np.concatenate([mean, std, abs_diff, minimum, maximum, max_min_diff, median, mad, iqr,\n",
        "                                    negative_count, positive_count, values_above_mean, peak_counts, skewness, kurt, energy,\n",
        "                                    gyro_avg_result, acc_avg_result, gyro_sma, acc_sma])\n",
        "  \n",
        "    return temp_features.tolist()\n",
        "\n",
        "train_df = new_df\n",
        "\n",
        "X_train_feat = []\n",
        "X_test_feat = []\n",
        "\n",
        "for i in range(X_train.shape[0]):\n",
        "  row = X_train.iloc[[i], :].values.reshape(-1, 6)\n",
        "  feature_vec = extract_features(pd.DataFrame(row))\n",
        "  X_train_feat.append(feature_vec)\n",
        "\n",
        "for i in range(X_test.shape[0]):\n",
        "  row = X_test.iloc[[i], :].values.reshape(-1, 6)\n",
        "  feature_vec = extract_features(pd.DataFrame(row))\n",
        "  X_test_feat.append(feature_vec)\n",
        "\n",
        "X_train_feat = np.array(X_train_feat)\n",
        "X_test_feat = np.array(X_test_feat)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Apply StandardScaler to the sensor data\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train_feat)\n",
        "\n",
        "X_test_scaled = scaler.transform(X_test_feat)"
      ],
      "metadata": {
        "id": "5l3xgK90z0fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA exploration\n",
        "\n",
        "# create a PCA object\n",
        "pca = PCA()\n",
        "\n",
        "# fit the PCA object to the sensor data\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "\n",
        "# calculate the cumulative sum of the explained variance ratio\n",
        "cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# plot the cumulative sum of the explained variance ratio\n",
        "plt.plot(cumulative_variance_ratio)\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance ratio')\n",
        "plt.title('Explained Variance Ratio vs. Number of Components')\n",
        "plt.show()\n",
        "\n",
        "# create a DataFrame to store the cumulative explained variance ratio\n",
        "df_cumulative_variance_ratio = pd.DataFrame({'Number of Components': range(1, len(cumulative_variance_ratio)+1),\n",
        "                                             'Cumulative Explained Variance Ratio': cumulative_variance_ratio})\n",
        "\n",
        "# print the DataFrame\n",
        "df_cumulative_variance_ratio.head(40)"
      ],
      "metadata": {
        "id": "zoWPsoJE2E6H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04f6f823-aa59-4849-aad2-9aaac5b62f9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Number of Components  Cumulative Explained Variance Ratio\n",
              "0                      1                             0.273253\n",
              "1                      2                             0.435517\n",
              "2                      3                             0.524367\n",
              "3                      4                             0.592827\n",
              "4                      5                             0.637720\n",
              "5                      6                             0.676029\n",
              "6                      7                             0.710671\n",
              "7                      8                             0.739857\n",
              "8                      9                             0.764786\n",
              "9                     10                             0.785510\n",
              "10                    11                             0.802024\n",
              "11                    12                             0.818347\n",
              "12                    13                             0.832222\n",
              "13                    14                             0.844603\n",
              "14                    15                             0.856274\n",
              "15                    16                             0.866354\n",
              "16                    17                             0.875963\n",
              "17                    18                             0.885112\n",
              "18                    19                             0.892949\n",
              "19                    20                             0.900209\n",
              "20                    21                             0.906603\n",
              "21                    22                             0.912827\n",
              "22                    23                             0.918481\n",
              "23                    24                             0.923851\n",
              "24                    25                             0.928750\n",
              "25                    26                             0.933391\n",
              "26                    27                             0.937932\n",
              "27                    28                             0.942029\n",
              "28                    29                             0.946043\n",
              "29                    30                             0.949847\n",
              "30                    31                             0.953496\n",
              "31                    32                             0.956715\n",
              "32                    33                             0.959908\n",
              "33                    34                             0.962807\n",
              "34                    35                             0.965460\n",
              "35                    36                             0.967766\n",
              "36                    37                             0.970010\n",
              "37                    38                             0.972131\n",
              "38                    39                             0.974075\n",
              "39                    40                             0.975900"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-69b6184e-e544-4853-8e35-5861064f2979\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Number of Components</th>\n",
              "      <th>Cumulative Explained Variance Ratio</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.273253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.435517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.524367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.592827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.637720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>0.676029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>0.710671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>0.739857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>0.764786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>0.785510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>0.802024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>0.818347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>0.832222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>0.844603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>0.856274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>0.866354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>0.875963</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18</td>\n",
              "      <td>0.885112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19</td>\n",
              "      <td>0.892949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>0.900209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>21</td>\n",
              "      <td>0.906603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>22</td>\n",
              "      <td>0.912827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>23</td>\n",
              "      <td>0.918481</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24</td>\n",
              "      <td>0.923851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25</td>\n",
              "      <td>0.928750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>26</td>\n",
              "      <td>0.933391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27</td>\n",
              "      <td>0.937932</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>28</td>\n",
              "      <td>0.942029</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29</td>\n",
              "      <td>0.946043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30</td>\n",
              "      <td>0.949847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>31</td>\n",
              "      <td>0.953496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>32</td>\n",
              "      <td>0.956715</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>33</td>\n",
              "      <td>0.959908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>34</td>\n",
              "      <td>0.962807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>35</td>\n",
              "      <td>0.965460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>36</td>\n",
              "      <td>0.967766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>37</td>\n",
              "      <td>0.970010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>38</td>\n",
              "      <td>0.972131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>39</td>\n",
              "      <td>0.974075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>40</td>\n",
              "      <td>0.975900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-69b6184e-e544-4853-8e35-5861064f2979')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-69b6184e-e544-4853-8e35-5861064f2979 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-69b6184e-e544-4853-8e35-5861064f2979');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjhzcMD8SIGX",
        "outputId": "9ea647ec-c98b-4fbd-8227-f61ede30a13a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.27325316 0.16226377 0.08885014 0.06845995 0.04489309 0.03830906\n",
            " 0.03464184 0.02918607 0.02492842 0.0207248  0.01651393 0.01632305\n",
            " 0.01387514 0.01238093 0.01167013 0.01008022 0.00960929 0.00914854\n",
            " 0.0078368  0.00725901 0.00639432 0.00622025 0.00565064 0.00534956]\n",
            "Number of components needed to explain K% of variance: 24\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# create a PCA object with n_components = K\n",
        "pca = PCA(n_components= 24 )\n",
        "\n",
        "# fit the PCA object to the sensor data\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "\n",
        "X_test_pca = pca.transform(X_test_scaled)\n",
        "\n",
        "# print the explained variance ratio for each component\n",
        "print(pca.explained_variance_ratio_)\n",
        "\n",
        "# Get the number of components needed to explain K% of the variance\n",
        "n_components_needed = pca.n_components_\n",
        "print(\"Number of components needed to explain K% of variance:\", n_components_needed)\n",
        "\n",
        "# compare top k eigenvalues and top k eigenvectors \n",
        "pca_eigvecs = pca.components_[:n_components_needed]\n",
        "pca_eigvals = pca.explained_variance_[:n_components_needed]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPmrcTJuHHj3"
      },
      "outputs": [],
      "source": [
        "# Define MLP model\n",
        "model = Sequential()\n",
        "model.add(Dense(16, input_dim=24, activation='relu'))\n",
        "model.add(Dense(8, activation='relu'))\n",
        "model.add(Dense(4, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rHq70T6HLqX",
        "outputId": "8badd691-e454-4ed2-df1f-46c2e92c8d8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "22/22 [==============================] - 2s 24ms/step - loss: 0.0768 - accuracy: 0.9912 - val_loss: 0.1694 - val_accuracy: 0.9474\n",
            "Epoch 2/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0534 - accuracy: 0.9956 - val_loss: 0.1466 - val_accuracy: 0.9474\n",
            "Epoch 3/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0397 - accuracy: 0.9971 - val_loss: 0.1339 - val_accuracy: 0.9474\n",
            "Epoch 4/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0314 - accuracy: 0.9971 - val_loss: 0.1265 - val_accuracy: 0.9474\n",
            "Epoch 5/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0257 - accuracy: 0.9971 - val_loss: 0.1198 - val_accuracy: 0.9474\n",
            "Epoch 6/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0219 - accuracy: 0.9971 - val_loss: 0.1118 - val_accuracy: 0.9474\n",
            "Epoch 7/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0187 - accuracy: 0.9971 - val_loss: 0.1083 - val_accuracy: 0.9474\n",
            "Epoch 8/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0163 - accuracy: 0.9985 - val_loss: 0.1053 - val_accuracy: 0.9605\n",
            "Epoch 9/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0145 - accuracy: 0.9985 - val_loss: 0.1031 - val_accuracy: 0.9605\n",
            "Epoch 10/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0130 - accuracy: 0.9985 - val_loss: 0.1024 - val_accuracy: 0.9605\n",
            "Epoch 11/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0119 - accuracy: 0.9985 - val_loss: 0.1014 - val_accuracy: 0.9605\n",
            "Epoch 12/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0109 - accuracy: 0.9985 - val_loss: 0.1009 - val_accuracy: 0.9605\n",
            "Epoch 13/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.1003 - val_accuracy: 0.9605\n",
            "Epoch 14/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0093 - accuracy: 0.9985 - val_loss: 0.0983 - val_accuracy: 0.9737\n",
            "Epoch 15/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 0.9985 - val_loss: 0.0965 - val_accuracy: 0.9737\n",
            "Epoch 16/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0956 - val_accuracy: 0.9737\n",
            "Epoch 17/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0929 - val_accuracy: 0.9737\n",
            "Epoch 18/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0925 - val_accuracy: 0.9737\n",
            "Epoch 19/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0915 - val_accuracy: 0.9737\n",
            "Epoch 20/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0907 - val_accuracy: 0.9737\n",
            "Epoch 21/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0884 - val_accuracy: 0.9737\n",
            "Epoch 22/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0061 - accuracy: 0.9985 - val_loss: 0.0887 - val_accuracy: 0.9737\n",
            "Epoch 23/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.0874 - val_accuracy: 0.9737\n",
            "Epoch 24/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0056 - accuracy: 0.9985 - val_loss: 0.0861 - val_accuracy: 0.9737\n",
            "Epoch 25/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.0860 - val_accuracy: 0.9737\n",
            "Epoch 26/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0052 - accuracy: 0.9985 - val_loss: 0.0847 - val_accuracy: 0.9737\n",
            "Epoch 27/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0845 - val_accuracy: 0.9737\n",
            "Epoch 28/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0828 - val_accuracy: 0.9737\n",
            "Epoch 29/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0823 - val_accuracy: 0.9737\n",
            "Epoch 30/100\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.0046 - accuracy: 0.9985 - val_loss: 0.0826 - val_accuracy: 0.9737\n",
            "Epoch 31/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0042 - accuracy: 0.9985 - val_loss: 0.0773 - val_accuracy: 0.9737\n",
            "Epoch 32/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0047 - accuracy: 0.9985 - val_loss: 0.0783 - val_accuracy: 0.9737\n",
            "Epoch 33/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.0798 - val_accuracy: 0.9737\n",
            "Epoch 34/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0039 - accuracy: 0.9985 - val_loss: 0.0848 - val_accuracy: 0.9737\n",
            "Epoch 35/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.0824 - val_accuracy: 0.9737\n",
            "Epoch 36/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.0820 - val_accuracy: 0.9737\n",
            "Epoch 37/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0038 - accuracy: 0.9985 - val_loss: 0.0791 - val_accuracy: 0.9737\n",
            "Epoch 38/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0037 - accuracy: 0.9985 - val_loss: 0.0792 - val_accuracy: 0.9737\n",
            "Epoch 39/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0038 - accuracy: 0.9985 - val_loss: 0.0787 - val_accuracy: 0.9737\n",
            "Epoch 40/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.9985 - val_loss: 0.0788 - val_accuracy: 0.9737\n",
            "Epoch 41/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0034 - accuracy: 0.9985 - val_loss: 0.0784 - val_accuracy: 0.9737\n",
            "Epoch 42/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0033 - accuracy: 0.9985 - val_loss: 0.0781 - val_accuracy: 0.9737\n",
            "Epoch 43/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0033 - accuracy: 0.9985 - val_loss: 0.0773 - val_accuracy: 0.9737\n",
            "Epoch 44/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0032 - accuracy: 0.9985 - val_loss: 0.0774 - val_accuracy: 0.9737\n",
            "Epoch 45/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0031 - accuracy: 0.9985 - val_loss: 0.0763 - val_accuracy: 0.9737\n",
            "Epoch 46/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0030 - accuracy: 0.9985 - val_loss: 0.0764 - val_accuracy: 0.9737\n",
            "Epoch 47/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 0.9985 - val_loss: 0.0765 - val_accuracy: 0.9737\n",
            "Epoch 48/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0031 - accuracy: 0.9985 - val_loss: 0.0771 - val_accuracy: 0.9737\n",
            "Epoch 49/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.9985 - val_loss: 0.0760 - val_accuracy: 0.9737\n",
            "Epoch 50/100\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0786 - val_accuracy: 0.9737\n",
            "Epoch 51/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0797 - val_accuracy: 0.9737\n",
            "Epoch 52/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0027 - accuracy: 0.9985 - val_loss: 0.0807 - val_accuracy: 0.9737\n",
            "Epoch 53/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 0.9985 - val_loss: 0.0811 - val_accuracy: 0.9737\n",
            "Epoch 54/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0026 - accuracy: 0.9985 - val_loss: 0.0825 - val_accuracy: 0.9737\n",
            "Epoch 55/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 0.9985 - val_loss: 0.0824 - val_accuracy: 0.9737\n",
            "Epoch 56/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0025 - accuracy: 0.9985 - val_loss: 0.0811 - val_accuracy: 0.9737\n",
            "Epoch 57/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.9985 - val_loss: 0.0823 - val_accuracy: 0.9737\n",
            "Epoch 58/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0024 - accuracy: 0.9985 - val_loss: 0.0826 - val_accuracy: 0.9737\n",
            "Epoch 59/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 0.9985 - val_loss: 0.0824 - val_accuracy: 0.9737\n",
            "Epoch 60/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0023 - accuracy: 0.9985 - val_loss: 0.0811 - val_accuracy: 0.9737\n",
            "Epoch 61/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0022 - accuracy: 0.9985 - val_loss: 0.0809 - val_accuracy: 0.9737\n",
            "Epoch 62/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 0.9985 - val_loss: 0.0813 - val_accuracy: 0.9605\n",
            "Epoch 63/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0022 - accuracy: 0.9985 - val_loss: 0.0825 - val_accuracy: 0.9605\n",
            "Epoch 64/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0021 - accuracy: 0.9985 - val_loss: 0.0821 - val_accuracy: 0.9605\n",
            "Epoch 65/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0821 - val_accuracy: 0.9605\n",
            "Epoch 66/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0817 - val_accuracy: 0.9605\n",
            "Epoch 67/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0819 - val_accuracy: 0.9605\n",
            "Epoch 68/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0826 - val_accuracy: 0.9605\n",
            "Epoch 69/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0830 - val_accuracy: 0.9605\n",
            "Epoch 70/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0836 - val_accuracy: 0.9605\n",
            "Epoch 71/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0827 - val_accuracy: 0.9605\n",
            "Epoch 72/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0829 - val_accuracy: 0.9605\n",
            "Epoch 73/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0821 - val_accuracy: 0.9605\n",
            "Epoch 74/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0825 - val_accuracy: 0.9605\n",
            "Epoch 75/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0821 - val_accuracy: 0.9605\n",
            "Epoch 76/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0824 - val_accuracy: 0.9605\n",
            "Epoch 77/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0826 - val_accuracy: 0.9605\n",
            "Epoch 78/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0830 - val_accuracy: 0.9605\n",
            "Epoch 79/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0822 - val_accuracy: 0.9605\n",
            "Epoch 80/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0833 - val_accuracy: 0.9605\n",
            "Epoch 81/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0838 - val_accuracy: 0.9605\n",
            "Epoch 82/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0812 - val_accuracy: 0.9605\n",
            "Epoch 83/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0823 - val_accuracy: 0.9605\n",
            "Epoch 84/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0822 - val_accuracy: 0.9605\n",
            "Epoch 85/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0821 - val_accuracy: 0.9605\n",
            "Epoch 86/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0821 - val_accuracy: 0.9605\n",
            "Epoch 87/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0834 - val_accuracy: 0.9605\n",
            "Epoch 88/100\n",
            "22/22 [==============================] - 0s 3ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0832 - val_accuracy: 0.9605\n",
            "Epoch 89/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0834 - val_accuracy: 0.9605\n",
            "Epoch 90/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0830 - val_accuracy: 0.9605\n",
            "Epoch 91/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0846 - val_accuracy: 0.9605\n",
            "Epoch 92/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0850 - val_accuracy: 0.9605\n",
            "Epoch 93/100\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0858 - val_accuracy: 0.9605\n",
            "Epoch 94/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0851 - val_accuracy: 0.9605\n",
            "Epoch 95/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0850 - val_accuracy: 0.9605\n",
            "Epoch 96/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0849 - val_accuracy: 0.9605\n",
            "Epoch 97/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 9.5608e-04 - accuracy: 1.0000 - val_loss: 0.0849 - val_accuracy: 0.9605\n",
            "Epoch 98/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 9.5125e-04 - accuracy: 1.0000 - val_loss: 0.0851 - val_accuracy: 0.9605\n",
            "Epoch 99/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 9.3046e-04 - accuracy: 1.0000 - val_loss: 0.0849 - val_accuracy: 0.9605\n",
            "Epoch 100/100\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 9.4021e-04 - accuracy: 1.0000 - val_loss: 0.0859 - val_accuracy: 0.9605\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0201bf9b80>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Compile model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train_pca, y_train, epochs=100, batch_size=32, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47WBG6gpYJbQ"
      },
      "outputs": [],
      "source": [
        "# hyperparameter tuning\n",
        "# from keras.wrappers.scikit_learn import KerasClassifier\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# # Define function to create model\n",
        "# def create_model(nodes=120, layers=3, optimizer='adam', activation='relu'):\n",
        "#     model = Sequential()\n",
        "#     model.add(Dense(nodes, input_dim=X_train.shape[1], activation=activation))\n",
        "#     for i in range(layers-1):\n",
        "#         model.add(Dense(nodes // (2 ** i), activation=activation))\n",
        "#     model.add(Dense(4, activation='softmax'))\n",
        "#     model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "#     return model\n",
        "\n",
        "# # Wrap Keras model for use with GridSearchCV\n",
        "# keras_model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# # Define hyperparameters to search over\n",
        "# param_grid = {'nodes': [64, 128, 256],\n",
        "#               'layers': [2, 3, 4, 5, 6],\n",
        "#               'optimizer': ['adam', 'rmsprop'],\n",
        "#               'activation': ['relu', 'sigmoid']}\n",
        "\n",
        "# # Use GridSearchCV to search over hyperparameters\n",
        "# grid = GridSearchCV(estimator=keras_model, param_grid=param_grid, cv=3)\n",
        "# grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# # Print best hyperparameters and accuracy\n",
        "# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7-Fu1WIbTQS"
      },
      "outputs": [],
      "source": [
        "# hyperparameter tuning\n",
        "# # Get the best hyperparameters\n",
        "# best_params = grid_result.best_params_\n",
        "\n",
        "# # Create the model with the best hyperparameters\n",
        "# model = create_model(nodes=best_params['nodes'], layers=best_params['layers'],\n",
        "#                      optimizer=best_params['optimizer'], activation=best_params['activation'])\n",
        "\n",
        "# # Train the model on the data\n",
        "# model.fit(X_train, y_train, epochs=200, batch_size=32, validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cialvt4-HNvQ"
      },
      "outputs": [],
      "source": [
        "# Evaluate model on testing data\n",
        "# loss, accuracy = model.evaluate(X_test_pca, y_test)\n",
        "# print('Test loss:', loss)\n",
        "# print('Test accuracy:', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOLNpniPHtPW"
      },
      "source": [
        "----------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZdDbv9x7sq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3489af88-16a8-4fd2-a9f9-2b2b4a9eaad8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/numpy/lib/npyio.py:713: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  val = np.asanyarray(val)\n"
          ]
        }
      ],
      "source": [
        "mean = scaler.mean_\n",
        "scale = scaler.scale_\n",
        "weights = [w.tolist() for w in model.get_weights()]\n",
        "\n",
        "mean_vec = pca.mean_\n",
        "# pca_eigvecs = pca.components_[:n_components_needed]\n",
        "\n",
        "np.savez('features_v1.6.npz', mean=mean, scale=scale, mean_vec=mean_vec, pca_eigvecs=pca_eigvecs, weights_list=weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ve7WELtoWRcc"
      },
      "outputs": [],
      "source": [
        "# Define MLP - 3 layers\n",
        "def mlp_math(X):\n",
        "    H1 = np.dot(X, weights[0]) + weights[1]\n",
        "    H1_relu = np.maximum(0, H1)\n",
        "    H2 = np.dot(H1_relu, weights[2]) + weights[3]\n",
        "    H2_relu = np.maximum(0, H2)\n",
        "    Y = np.dot(H2_relu, weights[4]) + weights[5]\n",
        "    Y_softmax = np.exp(Y) / np.sum(np.exp(Y))\n",
        "    return Y_softmax\n",
        "\n",
        "def get_action(softmax_array):\n",
        "    max_index = np.argmax(softmax_array)\n",
        "    action_dict = {0: 'G', 1: 'L', 2: 'R', 3: 'S'}\n",
        "    action = action_dict[max_index]\n",
        "    return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU5WjH99kclG"
      },
      "outputs": [],
      "source": [
        "# sanity check\n",
        "\n",
        "s,c = 0,0\n",
        "\n",
        "LOWER = 200\n",
        "UPPER = LOWER+300\n",
        "for ROW_NUM in range(LOWER, UPPER+1):\n",
        "  sanity_df = new_df.iloc[ROW_NUM]\n",
        "  sanity_label = sanity_df.iloc[-1]\n",
        "  sanity_data = sanity_df[:180].values.reshape(1,-1)\n",
        "\n",
        "  scaled_action_df = pd.DataFrame(sanity_data.reshape(-1,6))\n",
        "\n",
        "  # 1. Feature extraction\n",
        "  feature_vec = np.array(extract_features(scaled_action_df)).reshape(1,-1)\n",
        "\n",
        "  # 2. Scaler using features\n",
        "  scaled_action = scaler.transform(feature_vec)\n",
        "  scaled_action_math = (feature_vec - mean) / scale\n",
        "\n",
        "  tolerance = 1e-5\n",
        "  assert np.allclose(scaled_action.astype(float), scaled_action_math.astype(float), atol=tolerance)\n",
        "\n",
        "  # 3. PCA using scaler\n",
        "  pca_vec = (pca.transform(scaled_action)).astype(float)\n",
        "  pca_test_centered = scaled_action_math - mean_vec.reshape(1,-1)\n",
        "  pca_vec_math = np.dot(pca_test_centered, pca_eigvecs.T).astype(float)\n",
        "\n",
        "  assert np.allclose(pca_vec, pca_vec_math, atol=tolerance, rtol=tolerance)\n",
        "\n",
        "  # 4. MLP using PCA\n",
        "  pred = model.predict(np.array(pca_vec).reshape(1,-1))\n",
        "  pred_math = mlp_math(np.array(pca_vec_math).reshape(1,-1))\n",
        "\n",
        "  tol = 1e-2\n",
        "  assert np.allclose(pred, pred_math, atol=tol, rtol=tol)\n",
        "\n",
        "  action = get_action(pred)\n",
        "  action_math = get_action(pred_math)\n",
        "\n",
        "  assert action == action_math\n",
        "\n",
        "  if sanity_label == action_math:\n",
        "    s += 1\n",
        "\n",
        "  c +=1\n",
        "\n",
        "  print(action, action_math, sanity_label)\n",
        "\n",
        "print(s)\n",
        "print(c)\n",
        "print(s*100/c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xsjLLWCZyLu",
        "outputId": "6acb27ea-f002-4279-b44d-2bf4c4ebb334"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "[[4.70722915e-06 5.66195865e-05 4.53160321e-06 9.99934142e-01]]\n",
            "S\n"
          ]
        }
      ],
      "source": [
        "# # jupyter side sanity case checker\n",
        "\n",
        "# def test_case_checker(test_case):\n",
        "#   sanity_data = test_case.reshape(1,-1)\n",
        "\n",
        "#   scaled_action_df = pd.DataFrame(sanity_data.reshape(-1,6))\n",
        "\n",
        "#   # 1. Feature extraction\n",
        "#   feature_vec = np.array(extract_features(scaled_action_df)).reshape(1,-1)\n",
        "\n",
        "#   # 2. Scaler using features\n",
        "#   scaled_action = scaler.transform(feature_vec)\n",
        "#   scaled_action_math = (feature_vec - mean) / scale\n",
        "\n",
        "#   tolerance = 1e-5\n",
        "#   assert np.allclose(scaled_action.astype(float), scaled_action_math.astype(float), atol=tolerance)\n",
        "\n",
        "#   # 3. PCA using scaler\n",
        "#   pca_vec = (pca.transform(scaled_action)).astype(float)\n",
        "#   pca_test_centered = scaled_action_math - mean_vec.reshape(1,-1)\n",
        "#   pca_vec_math = np.dot(pca_test_centered, pca_eigvecs.T).astype(float)\n",
        "\n",
        "#   assert np.allclose(pca_vec, pca_vec_math, atol=tolerance, rtol=tolerance)\n",
        "\n",
        "#   # 4. MLP using PCA\n",
        "#   pred = model.predict(np.array(pca_vec).reshape(1,-1))\n",
        "#   pred_math = mlp_math(np.array(pca_vec_math).reshape(1,-1))\n",
        "\n",
        "#   tol = 1e-2\n",
        "#   assert np.allclose(pred, pred_math, atol=tol, rtol=tol)\n",
        "\n",
        "#   action = get_action(pred)\n",
        "#   action_math = get_action(pred_math)\n",
        "\n",
        "#   assert action == action_math\n",
        "\n",
        "#   print(pred_math)\n",
        "#   print(action_math)\n",
        "\n",
        "# test_case = test_s\n",
        "# test_case_checker(test_case)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity case checker to compare with cpp code\n",
        "\n",
        "def test_case_checker(test_case):\n",
        "  sanity_data = test_case.reshape(1,-1)\n",
        "\n",
        "  scaled_action_df = pd.DataFrame(sanity_data.reshape(-1,6))\n",
        "\n",
        "  # 1. Feature extraction\n",
        "  feature_vec = np.array(extract_features(scaled_action_df)).reshape(1,-1)\n",
        "\n",
        "  demo = feature_vec.reshape(-1,1)\n",
        "\n",
        "  with open('test_data.txt', 'w') as f:\n",
        "    # Write mean\n",
        "    f.write('float test_s[] = {')\n",
        "    for i in range(len(demo)):\n",
        "        f.write('%.16f' % demo[i])\n",
        "        if i != len(demo) - 1:\n",
        "            f.write(', ')\n",
        "    f.write('};\\n\\n')\n",
        "\n",
        "  # 2. Scaler using features\n",
        "  scaled_action_math = (feature_vec - mean) / scale\n",
        "\n",
        "  # 3. PCA using scaler\n",
        "  pca_test_centered = scaled_action_math - mean_vec.reshape(1,-1)\n",
        "  pca_vec_math = np.dot(pca_test_centered, pca_eigvecs.T).astype(float)\n",
        "\n",
        "  # 4. MLP using PCA\n",
        "  pred_math = mlp_math(np.array(pca_vec_math).reshape(1,-1))\n",
        "  action_math = get_action(pred_math)\n",
        "\n",
        "  print(pred_math)\n",
        "  print(action_math)\n",
        "\n",
        "test_case = test_s\n",
        "test_case_checker(test_case)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l9gN-Pt4sYv",
        "outputId": "4b72709f-a2b5-48f3-8166-58ed3c947a60"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.17604117e-06 5.88488803e-06 2.44145699e-08 9.99991915e-01]]\n",
            "S\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcfRryvQ2ARq"
      },
      "outputs": [],
      "source": [
        "# downloading to vivado\n",
        "# reset computer runtime and run these files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "N-li7xgrzdcO"
      },
      "outputs": [],
      "source": [
        "features = np.load('/content/features_v1.6.npz', allow_pickle=True)\n",
        "\n",
        "pca_eigvecs = features['pca_eigvecs']\n",
        "weights = features['weights_list']\n",
        "mean_vec = features['mean_vec']\n",
        "scale = features['scale']\n",
        "mean = features['mean']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "hxVmNNkq-RjL"
      },
      "outputs": [],
      "source": [
        "# # bitstream tools\n",
        "\n",
        "# Chop all values to 16 decimal places\n",
        "pca_eigvecs = np.round(pca_eigvecs, decimals=16)\n",
        "mean_vec = np.round(mean_vec, decimals=16)\n",
        "scale = np.round(scale, decimals=16)\n",
        "mean = np.round(mean, decimals=16)\n",
        "weights = [np.round(w, decimals=16) for w in weights]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "0WGXDuSHBHCf"
      },
      "outputs": [],
      "source": [
        "# Write the arrays to an output file\n",
        "with open('arrays_new.txt', 'w') as f:\n",
        "    # Write mean_vec\n",
        "    f.write('float mean_vec[] = {')\n",
        "    for i in range(len(mean_vec)):\n",
        "        f.write('%.16f' % mean_vec[i])\n",
        "        if i != len(mean_vec) - 1:\n",
        "            f.write(', ')\n",
        "    f.write('};\\n\\n')\n",
        "\n",
        "    # Write mean\n",
        "    f.write('float mean[] = {')\n",
        "    for i in range(len(mean)):\n",
        "        f.write('%.16f' % mean[i])\n",
        "        if i != len(mean) - 1:\n",
        "            f.write(', ')\n",
        "    f.write('};\\n\\n')\n",
        "\n",
        "    # Write scale\n",
        "    f.write('float scale[] = {')\n",
        "    for i in range(len(scale)):\n",
        "        f.write('%.16f' % scale[i])\n",
        "        if i != len(scale) - 1:\n",
        "            f.write(', ')\n",
        "    f.write('};\\n\\n')\n",
        "\n",
        "    # Write PCA eigenvectors\n",
        "    f.write('float pca_eigvecs[] = {')\n",
        "    for i in range(len(pca_eigvecs)):\n",
        "        for j in range(len(pca_eigvecs[i])):\n",
        "            f.write('%.16f' % pca_eigvecs[i][j])\n",
        "            if i != len(pca_eigvecs) - 1 or j != len(pca_eigvecs[i]) - 1:\n",
        "                f.write(', ')\n",
        "    f.write('};\\n\\n')\n",
        "\n",
        "    # Write weights\n",
        "    for k, w in enumerate(weights):\n",
        "      if k in [0, 2, 4]:\n",
        "          # Two-dimensional weight matrix\n",
        "          rows, cols = w.shape\n",
        "          f.write('float w%d[%d][%d] = {\\n' % (k, rows, cols))\n",
        "          for i in range(rows):\n",
        "              f.write('\\t{')\n",
        "              for j in range(cols):\n",
        "                  f.write('%.16f' % w[i][j])\n",
        "                  if j != cols - 1:\n",
        "                      f.write(', ')\n",
        "              f.write('}')\n",
        "              if i != rows - 1:\n",
        "                  f.write(',')\n",
        "              f.write('\\n')\n",
        "          f.write('};\\n\\n')\n",
        "      else:\n",
        "          # One-dimensional weight vector\n",
        "          f.write('float w%d[] = {' % k)\n",
        "          for i in range(len(w)):\n",
        "              f.write('%.16f' % w[i])\n",
        "              if i != len(w) - 1:\n",
        "                  f.write(', ')\n",
        "          f.write('};\\n\\n')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1WVA5U9xYkOEx0myOnbho_0CDdZw6dL5K",
      "authorship_tag": "ABX9TyMIpxuvcSv0GNCZXXWZXsGW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}